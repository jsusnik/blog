{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d2f9a8-1527-4e39-876f-d9a13e4e20e6",
   "metadata": {},
   "source": [
    "# A Personal Introduction: \n",
    "# Convolutional Neural Networks (CNN)\n",
    "###### John Susnik - October 7, 2021\n",
    "\n",
    "##### Tags: Convolutional Neural Networks (CNN), Image Recognition, Layering, Beginner\n",
    "\n",
    "### Brief Introduction:\n",
    "\n",
    "I am writing this blog to try and explain something I recently learned - and that I honestly may not fully understand, but this is my best attempt at explaining (in “layman's terms”) what a neural network is (more specifically: a convolutional neural network). I will try to keep the article light and casual.\n",
    "\n",
    "### Convolutional Neural Networks - What are they?\n",
    "\n",
    "I just recently learned what neural networks are, and some of their general uses. \n",
    "\n",
    "As a very brief introduction, Convolutional Neural Networks, or CNNs,  are most commonly used for analyzing visual imagery. They are structured as a pattern of convolutional layers and pooling layers, usually followed by a flatten layer, and one (or sometimes more) dense layers. I will get into the details of this later on below as this part can be very confusing.\n",
    "\n",
    "At this point I would like to take a step back and describe (in general terms) what these layers are doing. You can think of this process as a way for a computer to “visually” identify what pictures are. I put “visually” in quotations because one aspect of neural networks is that they are widely perceived as a “black box”, where the “purpose” of each layer is hard to conceptualize. \n",
    "\n",
    "Imagine you were handed an amazing painting but you were only allowed to look at this painting through a magnifying glass at a very close distance. Your vision would be limited to a very small area you would have to physically \"scan\" the entire painting to see everything. You may have a hard time understanding what the \"big picture\" is, and may even get frustrated at the thought of doing this. This is essentially how a computer, or in this case a CNN, looks at an image. Where a human would use a magnifying glass to analyze a painting, a CNN would \"read in\" each pixel and memorize the information while simulatenously \"describing\" the image. \n",
    "\n",
    "### What do you mean exactly by \"describing\" the image?\n",
    "\n",
    "In the same way that a human would take notes while inspecting a painting, such as:\n",
    "- the color\n",
    "- the texture\n",
    "- the lines\n",
    "- the brushwork\n",
    "- any other artistic feature\n",
    "- maybe you even want to record the time of day for each pixel as you inspect it\n",
    "- maybe you want to record the temperature of the room as well\n",
    "- maybe you make notes about what your favorite areas of the painting are\n",
    "\n",
    "A CNN is doing the same thing but a CNN speaks a different language. A CNN disects the image into multiple layers of numbers which are seemingly meaningless to us humans to understand - similar to how hard it would be for a human to explain to a computer how to \"feel happy\". When humans describe a painting as \"abstract\", a CNN can do the same thing but it is restricted to using numbers (the language of computers). So how does a CNN do this?\n",
    "\n",
    "Well a CNN might have an attribute column that describes this property (the property being \"how abstract is this image\") by using a value that ranges between -1 and 1, where -1 is abstract and 1 is realistic. If a human sees a cartoon of Scooby Doo, we would probably agree that this is not as abstract as a painting by Picasso, but also not as realistic as a picture of a family and their dog. It would make sense to us that a CNN might rate the \"abstract\" attribute for a Scooby Doo picture example as something like 0.25, it's not as abstract as a Picaso, but it's definitely not a realistic picture (remembering that a number close to 1 would be realistic, and a number close to -1 would be abstract) - **don't focus too much on the specific value (as its subjective), but more on the idea that we picked a number that is somewhere between -1 and 1 for this example.**\n",
    "\n",
    "### Okay, so how do CNNs actually work then?\n",
    "\n",
    "In the section above I explained (vaguely) how a CNN reads in images and how it describes the images. The tricky part to conceptualize is that while humans can make sense of things like \"color\", \"texture\", and \"realism\", a CNN doesn't know what any of these things are - it only knows numbers. When we send an image to a CNN we tell the CNN what type of layering process we want it to use, which is the architecture for how the CNN will describe the image - or learn about the image.\n",
    "\n",
    "When we provide an image to a CNN, we also provide a \"label\" that's associated with the image. For example if we want our CNN to distinguish the difference between cartoons and reality, the label might be \"1\" for a cartoon and \"0\" for a realistic picture. The CNN would look at the entire dataset of images that we provide it, and try to look for key differences between the 2 picture types. Similar to how a human could identify if a picture is a cartoon or a photograph of their brother, a CNN would look at the pixels of the image, the color-scale and line types, and create some numeric threshold to decide between the 2.\n",
    "\n",
    "As the CNN is \"learning\" about these images, it's trying to predict what the label will be of each picture, and then compares its prediction against the actual label, and will adjust its method for deciding. Using my example from above where I talked about \"abstract\" as a scale from -1 to 1, a CNN might say that any picture with an \"abstract\" value of 0 or lower is a cartoon, and anything that is greater than 0 is a real person or place. The tricky part here is that while I'm using this \"abstract\" feature as an example, in actuality we don't fully understand how a CNN classifies images. We only specify the \"architecture\" of the process (how many layers, how large the layers are, etc.).\n",
    "\n",
    "We tell a CNN to describe an image using 300 numbers, it will do its best to create its own categories to decide between the images itself. \n",
    "\n",
    "### You lost me again.\n",
    "\n",
    "Another example, imagine you have images of simple shapes (triangles, circles, squares) and each shape is one of three colors (red, green, or blue). You give this data to a CNN and tell it that there are 3 different colors, please separate the images by color (in this example, the color is the \"label\" or the \"target\" of our CNN model). The CNN would learn very quickly that a \"Red Square\" is classified as \"Red\" because all of the pixels (the RGB colors) are Red. Now you might think \"well that's a very easy task, of course a CNN could do that!\" but you might be surpised at how much time it may take for the CNN to distinguish a  \"Red Square\" from a \"Blue Circle\", because the CNN might try to learn about some other attribute of the images - such as the shape. What if the first 3 images that the CNN saw were a \"Red Square\", a \"Red Circle\" and a \"Red Triangle\"? The CNN might try to classify the 3 categories as the shape and not the color.\n",
    "\n",
    "### The  really tricky part.\n",
    "\n",
    "This is the part that can get REALLY confusing. I will try to keep things simple and not dive too much into the math or actual process, but more outline the challenges in designing a CNN.\n",
    "\n",
    "Continuing on my color and shape example from above, it might be inherently clear to us that if I give a CNN a group of images that consist of 3 shapes and 3 colors that I may want to tell it to classify the images in 6 different categories, one for each color and shape. What if I asked the CNN to classify between cartoons and reality? Well I would still want a label of either \"cartoon\" or \"reality\" for each picture, but how many attributes would a CNN need to differentiate between the 2 categories? This is the part that becomes difficult for humans to understand, and is the core challenge with CNNs.\n",
    "\n",
    "#### Look at this picture below, don't focus too much on the names of each layer, but understand that **Feature Maps or f.maps** are similar in concept to the \"attributes\" or columns I mention above.\n",
    "\n",
    "![Typical CNN](data/Typical_cnn.png) \n",
    "\n",
    "### Convolutional Layer \n",
    "\n",
    "In the beginning of the article I mention different types of layers (**some of which are shown in the picture above**). I won't dive into all of the layers, but I do want to explain the core layer for this particular model, and why it's used.\n",
    "\n",
    "The **Convolution Layer** puts the \"Convolutional\" into Convolutional Neural Networks. These layers are designed in such a way to provide the CNN a \"method\" for looking at an \"area\" or \"window\" of pixels at a time - rather than each pixel individually. This is what makes this type of layer ideal for image recognition - because to study and learn from an image, there has to be knowledge of spatial data. The CNN does this by starting out with a \"larger window\" (the left side of the picture) and then subsequently creates smaller and smaller windows, eventually getting to a very small size (let's say a size of 3x3 pixels), where the CNN will stop and eventually \"describe\" this very small area. Ultimately the CNN combines all of these descriptions - for all of these very tiny windows - which are still associated with the \"larger window\" and then proceed through the rest of the image. This is the magnifying glass analogy I mention above.\n",
    "\n",
    "##### [Wikipedia: Convolutional Layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer)\n",
    "\n",
    "Eventually, when all of these \"windows\" have been \"described\", the network culiminates into a final **fully connected**, or **dense layer** - this is similar to the list of \"attributes\" we mentioned above (color, texture, etc.). This **dense layer** takes the information from all of the convolutional layers, and assigns a value for whatever \"attributes\" the CNN has come up with - similar to my \"abstract\" attribute mentioned earlier. This is the stage where the CNN assigns a 0.25 value for the abstract \"attribute\".\n",
    "\n",
    "### Final questions and thoughts.\n",
    "\n",
    "- How many layers? \n",
    "- How large should each layer be? \n",
    "\n",
    "Think of the layer \"size\" as the number of attributes or columns we constrict the CNN for describing the image. I mention above that we stopped at a final \"window\" size of 3x3 pixels, maybe a larger window would be better, or maybe a smaller window? For something like simple shapes and colors, we may not need that many layers as the images are seemingly simple, but as the images get more detailed, where is the line? \n",
    "\n",
    "Using the shape and color example again, imagine we gave the CNN 100 columns or attributes to describe these images, and then told it to categorize the images. It's possible that the CNN would perform worse, because it may overcomplicate the process and start looking at things we didn't forsee. Maybe the CNN starts classifying the intensity of the color, the thickness of the lines around each shape, the angle at each interesction for each vertex on the triangles - the possibilities are endless. When the classification problem becomes more complicated, like deciding if an image is a cartoon or a photograph of a real place, how many layers should be used then?\n",
    "\n",
    "Another example: imagine you are being introduced to pizza for the first time and someone shows you hundreds of pictures of different types of pizza, and you are told to classify these pictures into 2 categories - but you don't fully understand what each category is, you just know there are 2 categories. You see pictures of square pizzas, deep-dish pizzas, pizzas with lots of toppings, some with no toppings at all, some that have BBQ sauce, some with tomato sauce. Where would you begin? Now imagine as you attempt to sort each pizza into the correct category, the person tells you \"correct!\" or \"wrong!\" without any additional information or explanation. You would have to repeat the process many times (sometimes hundreds, or thousands) before you might feel comfortable with the classification process. \n",
    "\n",
    "This is essentially how CNNs work.\n",
    "\n",
    "You may first try sorting by shape, maybe if that doesn't work you might try sorting by size, or maybe by color. Maybe you try and sort them every possible way you know how and you still don't get it right - luckily CNNs don't get frustrated like humans do! \n",
    "\n",
    "The trick is to find the \"sweet spot\" where the CNN can classify the images as accurate as possible, given the language barrier between humans and computers. \n",
    "\n",
    "<div>\n",
    "    <img src=\"data/ivan-torres-MQUqbmszGGM-unsplash.jpg\" width=\"500\"/>\n",
    "    <figcaption>Here's a picture of pizza for making it this far.</figcaption>\n",
    "</div>\n",
    "\n",
    "### Summary\n",
    "\n",
    "I hope this article helped explain the general idea behind Convolutional Neural Networks. I am not an expert on CNNs but after a few days of learning about their general framework I am very interested in how these layering architectures are constructed, and how to find the \"sweet spot\" for classifying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40ccca-5f4d-47f5-a167-a0d2eee047c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
